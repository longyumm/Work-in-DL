import torch 
from torch import nn
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as plt

torch.manual_seed(1)

TIME_STEP=10
INPUT_SIZE=1


steps=np.linspace(0,np.pi*2,100,dtype=np.float32)
x_np=np.sin(steps)
y_np=np.cos(steps)

class RNN(nn.Module):
	def __init__(self):
		super(RNN,self).__init__()

		self.rnn=nn.RNN(input_size=INPUT_SIZE,hidden_size=32,num_layers=1,batch_first=True)
		self.out=nn.Linear(32,1)

	def forward(self,x,h_state):
		#x(bacth*time_step*input_size)
		#h_state(n_layers,batch,hidden_size)
		#r_out(bacth*time_step*out_size)

		r_out, h_state=self.rnn(x, h_state)
		outs=[]

		for time_step in range(r_out.size(1)):
			outs.append(self.out(r_out[:,time_step,:]))

		return torch.stack(outs,dim=1),h_state

		##原始算法
		#r_out,h_state=self.rnn(x,h_state)
		#r_out_reshaped=r_out.view(-1,HIDDEN_SIZE)
		#outs=self.linear_layer(r_out_reshaped)
		#outs=outs.view(-1,TIME_STEP,INPUT_SIZE)

rnn=RNN()
print(rnn)

optimizer = torch.optim.Adam(rnn.parameters(),lr=0.01)
loss_func=nn.MSELoss()
h_state=None

for step in range(60):
	start,end=step*np.pi,(step+1)*np.pi
	steps=np.linspace(start,end,10,dtype=np.float32)
	x_np=np.sin(steps)
	y_np=np.cos(steps)

	x=Variable(torch.from_numpy(x_np[np.newaxis,:,np.newaxis]))
	y=Variable(torch.from_numpy(y_np[np.newaxis,:,np.newaxis]))

	prediction,h_state=rnn(x,h_state)

	h_state=Variable(h_state.data)

	loss=loss_func(prediction,y)
	optimizer.zero_grad()
	loss.backward()
	optimizer.step()

	plt.plot(steps,y_np.flatten(),'r-')
	plt.plot(steps,prediction.data.numpy().flatten(),('b-'))
	plt.draw()
	plt.pause(0.05)

plt.ioff()
plt.show()
